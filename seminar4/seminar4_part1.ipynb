{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 4: Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces, make_moons, make_circles\n",
    "from sklearn.datasets.samples_generator import make_swiss_roll\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel, sigmoid_kernel, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = make_moons(n_samples=500, noise=0.01, random_state=42)\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_std[y==0, 0], X_std[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_std[y==1, 0], X_std[y==1, 1], color='blue', alpha=0.25)\n",
    "plt.title('A nonlinear 2D dataset')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "pca_1d = PCA(n_components=1)\n",
    "X_pca_1d = pca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernels\n",
    "\n",
    "Kernel is any symmetric positive-definite function of the form:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$$\n",
    "\n",
    "where $\\phi$ is a function that projects vectors $\\mathbf{x}$ and $\\mathbf{x}'$ into a new vector space. The kernel function can be seen as a function that computes the inner-product between two vectors in some, potentially infinite-dimensinal space.\n",
    "\n",
    "### 1.1. Linear kernel\n",
    "\n",
    "A _linear kernel_ is given by $\\mathbf{x}^T\\mathbf{x}$, thus the inner product in the original vector space. Consider a $n \\times m$ dataset $\\mathbf{X} = \\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ with $n$ observations and $m$ features, then _Gram matrix_ $\\mathbf{G} = \\mathbf{X} \\mathbf{X}^T$ is a matrix of inner-products, in this notation covariance matrix $\\mathbf{S}$ is given by $\\mathbf{X}^T\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Compute the matrix of pairwise linear kernels for a moons dataset $\\mathbf{X}$, check that it is Gram matrix. You can use `linear_kernel` from `sklearn.metrics.pairwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KernelPCA with linear kernel obviously is equivalent to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with linear kernel\n",
    "kpca = KernelPCA(n_components=2)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w linear kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Gaussian kernel\n",
    "\n",
    "A radial basis function (RBF) is a real-valued function whose value depends only on the distance $d_X$ from some other point $x'$, so that $\\phi (\\mathbf {x} ,\\mathbf {x'} )= \\phi (\\|\\mathbf {x} -\\mathbf {x'} \\|)$.\n",
    "\n",
    "Many distances $d_X$ induce radial basis kernels which given by:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\exp \\left(- \\gamma \\| \\mathbf{x} -\\mathbf{x}' \\|^2\\right)$$\n",
    "\n",
    " If $\\gamma = 2 \\sigma^{-1}$ it is known as the Gaussian kernel:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\exp \\left(- \\frac{\\| \\mathbf{x} -\\mathbf{x}' \\|^2 }{2 \\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Compute the matrix of pairwise RBF kernels for a dataset $\\mathbf{X}$. Implement Gaussian kernel, check whether it values are equals scikit-learn solution. Check kernel values for different $\\gamma$ and $\\sigma$. You can use `rbf_kernel` from `sklearn.metrics.pairwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute K for X, matrix of paiwise RBF kernels for dataset X\n",
    "K = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Gaussian kernel\n",
    "def gaussian_kernel(x, x_prime, sigma=1):\n",
    "    # your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply KernelPCA with Gaussian kernel with different values of $\\gamma$ to Moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with Gaussian kernel\n",
    "gamma = 1\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=gamma)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"rbf\", gamma=gamma)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w Gaussian kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Polynomial kernel\n",
    "\n",
    "The polynomial kernel is defined as:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x'}) = (\\gamma \\mathbf{x}^\\top \\mathbf{x}' + c)^d$$\n",
    "\n",
    "where $\\mathbf{x}, \\mathbf{x'}$ are the input vectors, $\\gamma$, $c$ are scalars and $d$ is the kernel degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply KernelPCA with polynomial kernel to Moons dataset.\n",
    "\n",
    "Check how standartization affects embedding with polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with Gaussian kernel\n",
    "degree = 1\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"poly\", degree=degree)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"poly\", degree=degree)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w polynomial kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Cosine similarity\n",
    "\n",
    "Cosine similarity is the normalized dot product between vectors $\\mathbf{x}, \\mathbf{x}'$ which is the cosine of the angle between the points denoted by the vectors:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\frac{\\mathbf{x} \\mathbf{x}'^\\top}{\\|\\mathbf{x}\\| \\|\\mathbf{x}'\\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply Kernel PCA with cosine similarity `cosine_similarity` as precomputed kernel to Moons dataset.\n",
    "\n",
    "Check how standartization affects embedding with polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute K, matrix of paiwise cosine similarities of dataset X\n",
    "K = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with cosine similarity as precomputed kernel\n",
    "kpca = KernelPCA(n_components=2, kernel=\"precomputed\")\n",
    "X_kpca = kpca.fit_transform(K)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"precomputed\")\n",
    "X_kpca_1d = kpca_1d.fit_transform(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_std[y==0, 0], X_std[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_std[y==1, 0], X_std[y==1, 1], color='blue', alpha=0.25)\n",
    "#plt.scatter(X[y==0, 0], X[y==0, 1], color='green', alpha=0.25)\n",
    "#plt.scatter(X[y==1, 0], X[y==1, 1], color='orange', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w cosine similarity kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, color = make_swiss_roll(n_samples=1000, random_state=123)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.rainbow)\n",
    "plt.title('Swiss Roll in 3D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply Kernel PCA with different kernels and kernel parameters to Swiss roll dataset. Compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = # your code here\n",
    "X_kpca = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5.75))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_std[:, 0], X_std[:, 2], c=color, cmap=\"Reds\", alpha=0.25)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Kernel PCA /w Gaussian kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=color, cmap=\"Reds\", alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
